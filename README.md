# CTM

Colossal Trajectory Mining (CTM) detects co-movement patterns inside large scale trajectory datasets. A spatio-temporal references is partitioned in quanta, each quanta has a fixed spatial area and a temporal size(determined in hours). If the weekly flag is specified, then the temporal dimension is split on the days of the week and their hours. Then, for each cell is computed the set of trajectories whose at least one point is inside the cell bounds. After that, an extended version of the Carpenter algorithm is run to mine the clusters inside the dataset.  
 
## Input dataset

The input dataset must be a table accessible on a Hive installation, 
the table must have the at least the following columns: 

* _userid: String_ contains a custom ID for every point of a trajectory
* _trajectoryid: String_ contains the trajectory ID for every point of a trajectory
* _latitude: Double_ contains the latitude of a point
* _longitude: Double_ contains the longitude of a point
* _timestamp: Long_ contains seconds elapsed since 1/1/1970 (unix_time) as a Long

## Output results

Output result are stored on a Hive table named: `CTM__par1_val1__...__parN_valN.csv`

Also logs data are stored inside the folder `logs`.

## How to run this project

Unit tests are available via the `--debug` option on launch.

    git pull; \
    ./gradlew clean build shadowJar; \
    spark-submit --class it.unibo.big.CTM build/libs/CTM-all.jar --debug --droptable

Experimental tests are available by running:
    
    git pull; \
    ./gradlew clean build shadowJar; \
    rm results/CTM_stats.csv; \
    sh run_CTM.sh

Statistics are available in the `logs/` folder. `run_CTM.sh` is generated by `src/main/python/gen_run_CTM.py`

An example of experimental run is the following:

    ???

where: 

    ???

## Convert a TSV dataset to a valid CUTE one

There is a dedicated JAR to do this, generated by the task _conversionJar_
The dataset must be a valid tsv file and must contains all the fields necessary for the CUTE algorithm: 
__trajid__, __latitude__, __longitude__, __timestamp__ as mentioned above

To launch the conversion, few parameters are mandatory: 
   
```
spark-submit /path/to/jar/BIG-trajectory-conversion.jar \
    --sampling_ratio=5 \
    --input_file_path=\input\files\path \
    --output_path=\output\file\path\or\table\name \
    --writeonhive
```

where: 
 
* `--sampling_ratio=5` is the temporal scale unit of the original dataset expressed in seconds, for example if the original dataset sampled data every minute
(so the first minute has value 1, the second 2....) this parameter must be set to 60 in order to convert time in seconds, as required inside CUTE.
* `--input_file_path=\input\files\path` is the input file to process.
* `--writeonhive` is a boolean flag to determine if the table will be written on hive or stored as a TSV on HDFS.
* `--output_path=\output\file\path\or\table\name` is the output table name if the result is stored on hive, the HDFS path if the.
 `--writeonhive` flag is not specified.
* `--artificial` this flag will covert a dataset based on Oldenburg where the columns have a different order.  
     
## Export a valid CUTE dataset to tsv
There is a dedicated JAR to do this, generated by the task _exportTSVJar_
The dataset must be a valid tsv file and must contains all the fields necessary for the CUTE algorithm: 
__trajid__, __latitude__, __longitude__, __timestamp__ as mentioned above.
Create a tsv dataset composed of the following fields: __trajid__, __latitude__, __longitude__, __timestamp__ 

To launch the conversion, few parameters are mandatory: 
   
```
spark-submit /path/to/jar/BIG-trajectory-tsv-export.jar \
    --time_sampling_ratio=5 \
    --weeklyscale=daily \
    --input_table_name=\input\files\path \
    --output_tsv_path=\output\file\path\or\table\name
```

where:

* `--time_sampling_ratio=5` is the temporal scale unit of the original dataset expressed in seconds, for example if the original dataset sampled data every minute
(so the first minute has value 1, the second 2....) this parameter must be set to 60 in order to convert time in seconds, as required inside CUTE. It must be used only if 
`--weeklyscale` is not specified, never use both of them or none of them.
* `--weeklyscale=daily` if the time scale is not absolute, but in days of week or hour of the day, then use this flag.
* `--input_table_name=input_table_name` is the name of the table to export.
* `--output_tsv_path=\output\file\path\or\table\name` the HDFS path on which the dataset will be stored.
      
## Sample a dataset and store it as table and tsv

Starting from an Hive table with valid fields, generate a sampled on time and/or number of trajectories dataset on both table and tsv.
It's possible to limit the number of trajectories on the trajectory id.
It's necessary to specify the time unit for the temporal sampling.
The output is a filtered table and a valid TSV file. 
There is a dedicated JAR to do this, generated by the task _writerJar_.

To launch the conversion, few parameters are mandatory: 

```
spark-submit /path/to/jar/BIG-trajectory-conversion.jar \
    --time_sampling_ratio=5 \
    --input_table_name=geolife_bejin \
    --output_table_name=geolife_bejin_top100 \
    --output_tsv_path=tsv_dataset \
    --limit=100
```

where: 

* `--time_sampling_ratio=5` specify the time unit of the dataset in seconds.
* `--input_table_name=geolife_bejin` is the table input to process.
* `--output_table_name=geolife_bejin` is the output table name of the filtered input table.
* `--output_tsv_path=tsv_dataset` is the output path at which will be stored the TSV file.
* `--limit=100` specify the number of trajectories considered in the process.

## Compute similarity between results

For computing the similarity between two output results, is available a specific jar task _similarityJar_,
that will produce a fatjar who will read two output dataset as two separate array of string sets, each set being a cluster
The similarity is the calculated considering all the common set of string(so the trajectories must have the same trajectory
ids between the two dataset, or else the result will be compromised).
The result is stored inside a file with the following path: ```./comparison/CUTE_similarity.txt```

Command to run the jar is the following: 

```
spark-submit /path/to/jar/BIG-trajectory-similarity.jar \
    --input_table_name=inputtablename \
    --input_tsv_path=inputtsvpath \
```

where: 

* `--input_table_name=inputtablename` is the table input where the cute result are stored.
* `--input_tsv_path=tsv_dataset` is the output path of the other result, these must be stored inside a TSV file.